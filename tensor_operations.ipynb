{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4f592be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import scipy.linalg as LA \n",
    "# import copy\n",
    "# %matplotlib widget\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from ncon import ncon\n",
    "from timeit import default_timer as timer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c1817",
   "metadata": {},
   "source": [
    "# Tensor operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5646818",
   "metadata": {},
   "source": [
    "### E1: Defining generic tensors  \n",
    "Here we will start studying generic tensor networks and their contraction.\n",
    "\n",
    "Part of any tensor network study is to understand how to contract some of the parts of the tensor networks, and eventually simplify them. \n",
    "\n",
    "The first task is to initialize tensors, we did this already for vectors and tensors are no different. \n",
    "\n",
    "Let' s create \n",
    "\n",
    "- a random complex tensor with four legs (an order 4 tensor) with size 2, 3, 4, 5 and call it A\n",
    "- an order 3 tensor with size 4 5 6 and call it B\n",
    "- a 5x5 identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2de370d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(2,3,4,5) +1j*np.random.rand(2,3,4,5)\n",
    "B = np.random.rand(4,5,6) +1j*np.random.rand(4,5,6)\n",
    "C = np.eye(5)  # should be same as np.eye(5,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038b4ea",
   "metadata": {},
   "source": [
    "### E2: Some special tensors\n",
    "\n",
    "Other special tensors are those made by all ones, that can be multiplied by an arbitrary constant, thus obtaining a tensor where each element is the same.\n",
    "\n",
    "-Create one that has order 4 and dimension 2, 4, 2, 4, and call it D\n",
    "\n",
    "Finally when a tensor only has few non-zero elements, one can create a tensor made of all zeros and fill the desider elements.\n",
    "\n",
    "-Create a tensor with order 2 made of zero and fill the first element with a random complex number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58a40856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor of 1's, order 4, dims: 2-by-4-by-2-by-4\n",
    "D = np.ones((2,4,2,4), dtype=complex)\n",
    "D *= (np.random.rand()+1j*np.random.rand())\n",
    "\n",
    "# matrix of 0's, order 2, dims: 3-by-5\n",
    "F = np.zeros((3,5), dtype=complex)\n",
    "F[0,0] = np.random.rand()+1j*np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a1fb2",
   "metadata": {},
   "source": [
    "### E3: Permuting and reshaping \n",
    "\n",
    "Some common operations on tensors are \n",
    "\n",
    "-Reordering the legs, ie. permuting them, which incurs in a computational cost proportional to the size of the tensor\n",
    "\n",
    "-Grouping or splitting the legs, which does not have a relevant computational cost (for large tensors) since it only changes the labels used to address the elements \n",
    "\n",
    "For example implement the above permutation and reshaping  for the tensor A  and B defined in the figure \n",
    "\n",
    "<img src=\"reshape_permute.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2815e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.shape(A)=(2, 3, 4, 5)\n",
      "np.shape(At)=(5, 2, 3, 4)\n",
      "np.shape(Bt)=(4, 30)\n"
     ]
    }
   ],
   "source": [
    "At = A.transpose(3,0,1,2)\n",
    "Bt = B.reshape(4,30)\n",
    "\n",
    "print(f\"{np.shape(A)=}\")\n",
    "print(f\"{np.shape(At)=}\")\n",
    "print(f\"{np.shape(Bt)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b5a91",
   "metadata": {},
   "source": [
    "## Tensor contractions\n",
    "\n",
    "We now enter the realm of tensor contractions. \n",
    "\n",
    "First of all, the basic rule: contracting two tensors means summing the product of the corresponding tensor elements, it is a generalization of matrix multiplication\n",
    "\n",
    "$ M_{ik}  =\\sum_j A_{ij} * B_{jk}$\n",
    "\n",
    "We will start by contracting two tensors. This can be done in several ways.\n",
    "\n",
    "Define a new tensor $G$ with order 4 and dimensions 3,6,7,5\n",
    "\n",
    "Now contract it with the $A$ tensor with dimensions (2,3,4,5) defined above on the second and fourth leg, ie. \n",
    "\n",
    "$ K_{ijkl} = \\sum_{mn} A_{imjn} G_{mkln} $\n",
    "\n",
    "First compute it using for loops. Note that you need to loop over all the involved legs, hence the computational cost! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec68ed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4, 5) (3, 6, 7, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "G = np.random.rand(3,6,7,5) +1j*np.random.rand(3,6,7,5) \n",
    "\n",
    "K = np.zeros((A.shape[0],A.shape[2],G.shape[1],G.shape[2]), dtype=complex)\n",
    "\n",
    "print(np.shape(A), np.shape(G))\n",
    "\n",
    "for i in range(A.shape[0]):\n",
    "    for m in range(A.shape[1]):\n",
    "        for j in range(A.shape[2]):\n",
    "            for n in range(A.shape[3]):\n",
    "                for k in range(G.shape[1]):\n",
    "                    for ll in range(G.shape[2]):\n",
    "                        K[i,j,k,ll] += A[i,m,j,n]*G[m,k,ll,n]\n",
    "                       \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf13fa",
   "metadata": {},
   "source": [
    "### E4: Tensor products as matrix multiplications \n",
    "\n",
    "Now repeat the same operation by transforming the two tensors into matrix and then performing a matrix multiplication, \n",
    "you will need to transpose and reshape accordingly.\n",
    "\n",
    "Compare that the two methods provide the same result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e8e6041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ap = A.transpose(0,2,1,3)\n",
    "Gp = G.transpose(0,3,1,2)\n",
    "App = Ap.reshape(A.shape[0]*A.shape[2],A.shape[1]*A.shape[3])\n",
    "Gpp = Gp.reshape(G.shape[0]*G.shape[3],G.shape[1]*G.shape[2])\n",
    "Kpp = App @ Gpp            \n",
    "K_tilde= Kpp.reshape(A.shape[0],A.shape[2],G.shape[1],G.shape[2])\n",
    "\n",
    "np.allclose(K,K_tilde)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ec12e",
   "metadata": {},
   "source": [
    "### Contractions using einsum \n",
    "There are more tools to make our lives easier, such as np.einsum, which (as the name suggests)\n",
    "is inspired by the Einstein notation. We can make the same contraction above as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6cf4fdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_einsum = np.einsum(\"imjn,mkln->ijkl\", A, G)\n",
    "\n",
    "np.allclose(K,K_einsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ed673",
   "metadata": {},
   "source": [
    "### Contractions using ncon \n",
    "\n",
    "Finally, we can also use ncon (network contractor), which can also work with networks of more tensors.\n",
    "\n",
    "From the manual:\n",
    "\n",
    "Network contractor ‘ncon’:\n",
    "\n",
    "The ‘ncon’ function is a useful tool to lessen the programming effort required to implement a tensor network contraction. This function works by automatically performing a desired sequence of permutes, reshapes and matrix multiplications required to evaluate a tensor network. The ‘ncon’ code and detailed instructions for its usage can be found here, or alternatively the code is also presented on the example code page. The first step in using ‘ncon’ to evaluate a network is to make a labelled diagram of the network such that:​\n",
    "\n",
    "​\n",
    "\n",
    "Each internal index is labelled with a unique positive integer (typically sequential integers starting from 1, although this is not necessary).\n",
    "\n",
    "​\n",
    "\n",
    "External indices of the diagram (if there are any) are labelled with sequential negative integers [-1,-2,-3,…] which denote the desired index order on the final tensor (with -1 as the first index, -2 as the second etc).\n",
    "\n",
    "​\n",
    "\n",
    "Following this, the ‘ncon’ routine is called as follows,\n",
    "\n",
    "​\n",
    "\n",
    " \n",
    "\n",
    "OutputTensor = ncon(TensorArray, IndexArray),\n",
    "\n",
    "​\n",
    "\n",
    " \n",
    "\n",
    "with input arguments defined:\n",
    "\n",
    "TensorArray: 1D cell array containing the tensors comprising the network\n",
    "\n",
    "​\n",
    "\n",
    "IndexArray: 1D cell array of vectors, where the kth element is a vector of the integer labels from the diagram on the kth tensor from ‘TensorArray’ (ordered following the corresponding index order on this tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "884298c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_ncon = ncon([A,G], [[-1,1,-2,2],[1,-3,-4,2]])\n",
    "\n",
    "np.allclose(K_ncon, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4d314",
   "metadata": {},
   "source": [
    "### Computational cost of tensor contractions. \n",
    "\n",
    "As you have seen, contracting tensors comes at a cost.\n",
    "When your network to contract include more than two tensors it is computationally advantageous to break the contraction into pairwise contractions. For example if you need to contract three tensors, and you do it in a single shot (by using for loops) you incur into a higher computational cost. Try it with the simple matrix product A*B*C, with all three being d*d matrices and, eg. d=70\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b8df526",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 60\n",
    "A = np.random.rand(d,d) \n",
    "B = np.random.rand(d,d)\n",
    "C = np.random.rand(d,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91bd3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate network via summation over internal indices\n",
    "F0 = np.zeros((d,d))\n",
    "\n",
    "for di in range(d):\n",
    "    for dj in range(d):\n",
    "        for dk in range(d):\n",
    "            for dl in range(d):\n",
    "                F0[di,dj] += A[di,dk]*B[dk,dl]*C[dl,dj]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "143850db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate network via sequence of binary contractions\n",
    "F1 = (A @ B) @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b4f4b4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(F0,F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979fab2",
   "metadata": {},
   "source": [
    "### Some timing comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdf53df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57525366700429, 0.00010099999781232327, 0.0007081250078044832]\n"
     ]
    }
   ],
   "source": [
    "d = 10\n",
    "A = np.random.rand(d,d,d) \n",
    "B = np.random.rand(d,d,d)\n",
    "C = np.random.rand(d,d,d)\n",
    "\n",
    "##### Evaluate network via index summation\n",
    "def tempfunct(A,B,C,d):\n",
    "    D0 = np.zeros((d,d,d))\n",
    "    for b1 in range(d):\n",
    "        for a2 in range(d):\n",
    "            for c3 in range(d):\n",
    "                for a1 in range(d):\n",
    "                    for a3 in range(d):\n",
    "                        for c1 in range(d):\n",
    "                            D0[b1,a2,c3] = D0[b1,a2,c3]+A[a1,a2,a3]*B[b1,a1,c1]*C[c1,a3,c3]\n",
    "    return D0\n",
    "\n",
    "t0 = timer()\n",
    "D0 = tempfunct(A,B,C,d)\n",
    "t_sum = timer() - t0\n",
    "\n",
    "##### Evaluate network using reshape and permute\n",
    "def tempfunct2(A,B,C,d):\n",
    "    Xmid = (B.transpose(0,2,1).reshape(d**2,d) @ A.reshape(d,d**2)).reshape(d,d,d,d)\n",
    "    D1 = (Xmid.transpose(0,2,1,3).reshape(d**2,d**2) @ C.reshape(d**2,d)).reshape(d,d,d)\n",
    "    return D1\n",
    "\n",
    "t0 = timer()\n",
    "D1 = tempfunct2(A,B,C,d)\n",
    "t_res = timer() - t0\n",
    "\n",
    "##### Evaluate using ncon\n",
    "t0 = timer()\n",
    "D2 = ncon([A,B,C],[[1,-2,2],[-1,1,3],[3,2,-3]]) #, cont_order = [1,2,3])\n",
    "t_ncon = timer() - t0\n",
    "\n",
    "##### Compare\n",
    "tdiffs = [max(abs(D0-D1).flatten()),max(abs(D1-D2).flatten()),max(abs(D2-D0).flatten())]\n",
    "ttimes = [t_sum, t_res, t_ncon]\n",
    "print(ttimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782511c",
   "metadata": {},
   "source": [
    "### SVD and factorizations\n",
    "\n",
    "We can use SVDs (or other factorizations such as QR) to separate a single tensor into multiple ones.\n",
    "\n",
    "To get an idea, work with matrices. \n",
    "Build a random (40x28) matrix and SVD it. What is the maximum rank of the matrix ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ddd4c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 28) (28,) (28, 28)\n"
     ]
    }
   ],
   "source": [
    "M = np.random.rand(40,28) +1j*np.random.rand(40,28)\n",
    "u,s,vd = np.linalg.svd(M, full_matrices=False)\n",
    "print(np.shape(u),np.shape(s),np.shape(vd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cb6d4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 28) (28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,r = np.linalg.qr(M)\n",
    "print(np.shape(q),np.shape(r))\n",
    "np.allclose(q @ r ,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb9939",
   "metadata": {},
   "source": [
    "### Reconstruction \n",
    "\n",
    "Try to reconstruct the original tensor, see that nothing got lost. Use einsum or ncon\n",
    "\n",
    "Hint: recall that - while it is stored as a vector, S should be actually thougt of as a diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6771ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "sm = np.diag(s)\n",
    "print(np.shape(sm))\n",
    "\n",
    "M_ncon = ncon([u,sm,vd],[[-1,1],[1,2],[2,-2]])\n",
    "M_einsum = np.einsum(\"ij,jk,kl->il\",u,sm,vd)\n",
    "print(np.allclose(M_ncon,M))\n",
    "print(np.allclose(M_einsum,M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9143e52d",
   "metadata": {},
   "source": [
    "### Truncation \n",
    "\n",
    "We can also use SVDs to truncate though. For this, we can discard the smallest singular values. \n",
    "\n",
    "What will the error be if we try to reconstruct the tensor ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae3125bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.linalg.norm(s[cut:])=np.float64(3.0212465179305354)\n",
      "(20, 20)\n",
      "np.allclose(M_ncon,M)=False\n",
      "np.linalg.norm(M_ncon - M)=np.float64(3.0212465179305354)\n"
     ]
    }
   ],
   "source": [
    "cut = 20 \n",
    "\n",
    "sm = np.diag(s[:cut])\n",
    "\n",
    "print(f\"{np.linalg.norm(s[cut:])=}\")\n",
    "\n",
    "print(np.shape(sm))\n",
    "\n",
    "M_ncon = ncon([u[:,:cut],sm,vd[:cut,:]],[[-1,1],[1,2],[2,-2]])\n",
    "print(f\"{np.allclose(M_ncon,M)=}\")\n",
    "\n",
    "print(f\"{np.linalg.norm(M_ncon - M)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe08bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
